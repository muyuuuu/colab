{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOv7Zv31tEtvoa4oCqNhQHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muyuuuu/colab/blob/main/GAN/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytqwzzpjPb6"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGdJHlD-jUmH"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "\n",
        "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
        "        if mode == \"train\":\n",
        "            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        w, h = img.size\n",
        "        # 真实\n",
        "        img_A = img.crop((0, 0, w / 2, h))\n",
        "        # 彩色\n",
        "        img_B = img.crop((w / 2, 0, w, h))\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "\n",
        "        img_A = self.transform(img_A)\n",
        "        img_B = self.transform(img_B)\n",
        "\n",
        "        return {\"A\": img_A, \"B\": img_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njSo2YpKjgE4"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "##############################\n",
        "#           U-NET\n",
        "##############################\n",
        "\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_size))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.0):\n",
        "        super(UNetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # U-Net generator with skip connections from encoder to decoder\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "\n",
        "        return self.final(u7)\n",
        "\n",
        "\n",
        "##############################\n",
        "#        Discriminator\n",
        "##############################\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmahUqIjmFh"
      },
      "source": [
        "os.makedirs(\"images/facades\", exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion_GAN = torch.nn.MSELoss().cuda()\n",
        "criterion_pixelwise = torch.nn.L1Loss().cuda()\n",
        "\n",
        "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
        "lambda_pixel = 100\n",
        "\n",
        "\n",
        "generator = GeneratorUNet().cuda()\n",
        "discriminator = Discriminator().cuda()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INO-qNMXmA2p",
        "outputId": "256764c8-1951-4f86-b553-9ecb801c8955"
      },
      "source": [
        "! test -e data; mkdir data\n",
        "% cd data\n",
        "! wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
        "! tar -zxvf facades.tar.gz\n",
        "% cd.."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "/content/data\n",
            "--2021-08-09 12:34:17--  http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30168306 (29M) [application/x-gzip]\n",
            "Saving to: ‘facades.tar.gz’\n",
            "\n",
            "facades.tar.gz      100%[===================>]  28.77M  6.01MB/s    in 5.0s    \n",
            "\n",
            "2021-08-09 12:34:22 (5.79 MB/s) - ‘facades.tar.gz’ saved [30168306/30168306]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY8yoabCkulM",
        "outputId": "3cc1dce3-d041-4f6a-b72e-96a262652167"
      },
      "source": [
        "transforms_ = [\n",
        "    transforms.Resize((256, 256), Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(\"data/facades\", transforms_=transforms_),\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(\"data/facades\", transforms_=transforms_, mode=\"val\"),\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfQlHbFglT8_"
      },
      "source": [
        "def sample_images(batches_done):\n",
        "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    real_R = imgs[\"B\"].cuda()\n",
        "    real_L = imgs[\"A\"].cuda()\n",
        "    fake_R = generator(real_R)\n",
        "    print(real_R.size(), real_L.size(), fake_R.size())\n",
        "    img_sample = torch.cat((real_R.data, fake_R.data, real_L.data), -2)\n",
        "    save_image(img_sample, \"./images/facades/%s.png\" % batches_done, nrow=5, normalize=True)\n",
        "    print('here')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea-OI1GUo8F2",
        "outputId": "73ea7152-a06b-4828-d591-6c536d0fefe5"
      },
      "source": [
        "# 只判断一个 pacth\n",
        "patch = (1, 256 // 2 ** 4, 256 // 2 ** 4)\n",
        "n_epochs = 200\n",
        "\n",
        "\n",
        "for epoch in range(0, n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        # Model inputs\n",
        "        real_R = batch[\"B\"].cuda()\n",
        "        real_L = batch[\"A\"].cuda()\n",
        "\n",
        "        # [batchsize, 1, 16, 16]\n",
        "        valid = torch.ones((real_R.size(0), *patch)).cuda()\n",
        "        fake = torch.zeros((real_R.size(0), *patch)).cuda()\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # GAN loss\n",
        "\n",
        "        # 右侧\n",
        "        fake_R = generator(real_R)\n",
        "        # 虚假的\n",
        "        pred_fake = discriminator(fake_R, real_R)\n",
        "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "\n",
        "\n",
        "        # Pixel-wise loss\n",
        "        loss_pixel = criterion_pixelwise(fake_R, real_L)\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = discriminator(real_L, real_R)\n",
        "        loss_real = criterion_GAN(pred_real, valid)\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake = discriminator(fake_R.detach(), real_R)\n",
        "        loss_fake = criterion_GAN(pred_fake, fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D = 0.5 * (loss_real + loss_fake)\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "        # Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = n_epochs * len(dataloader) - batches_done\n",
        "\n",
        "        # Print log\n",
        "        print(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f]\"\n",
        "            % (\n",
        "                epoch,\n",
        "                n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_pixel.item(),\n",
        "                loss_GAN.item(),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # If at sample interval save image\n",
        "        if epoch == 0:\n",
        "            sample_images(batches_done)\n",
        "            break\n",
        "    break"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 1, 16, 16])\n",
            "\r[Epoch 0/200] [Batch 0/506] [D loss: 0.019100] [G loss: 32.095963, pixel: 0.311808, adv: 0.915175]\n",
            "torch.Size([10, 3, 256, 256]) torch.Size([10, 3, 256, 256]) torch.Size([10, 3, 256, 256])\n",
            "here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdzzoCbau636"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}